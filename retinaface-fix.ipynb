{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "f8cb7aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\"\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gdown\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "\n",
    "from typing import Union, Any, Optional, Dict, Tuple, List\n",
    "import numpy as np\n",
    "from retinafacelib import preprocess\n",
    "from retinafacelib import postprocess\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "d9c8dd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Recreate the EXACT model architecture from the codebase\n",
    "def build_retinaface_model():\n",
    "    \"\"\"\n",
    "    This is the EXACT same architecture from retinaface_model.py\n",
    "    Every layer name, parameter, and connection must match perfectly\n",
    "    \"\"\"\n",
    "    \n",
    "    # Import the correct Keras modules based on TensorFlow version\n",
    "    tf_version = int(tf.__version__.split(\".\", maxsplit=1)[0])\n",
    "    \n",
    "    if tf_version == 1:\n",
    "        from keras.models import Model\n",
    "        from keras.layers import (\n",
    "            Input, BatchNormalization, ZeroPadding2D, Conv2D, ReLU,\n",
    "            MaxPool2D, Add, UpSampling2D, concatenate, Softmax,\n",
    "        )\n",
    "    else:\n",
    "        from tensorflow.keras.models import Model\n",
    "        from tensorflow.keras.layers import (\n",
    "            Input, BatchNormalization, ZeroPadding2D, Conv2D, ReLU,\n",
    "            MaxPool2D, Add, UpSampling2D, concatenate, Softmax,\n",
    "        )\n",
    "    \n",
    "    # START: Copy the EXACT architecture from lines 99-1516\n",
    "    data = Input(dtype=tf.float32, shape=(None, None, 3), name=\"data\")\n",
    "\n",
    "    bn_data = BatchNormalization(epsilon=1.9999999494757503e-05, name=\"bn_data\", trainable=False)(\n",
    "        data\n",
    "    )\n",
    "\n",
    "    conv0_pad = ZeroPadding2D(padding=tuple([3, 3]))(bn_data)\n",
    "\n",
    "    conv0 = Conv2D(\n",
    "        filters=64,\n",
    "        kernel_size=(7, 7),\n",
    "        name=\"conv0\",\n",
    "        strides=[2, 2],\n",
    "        padding=\"VALID\",\n",
    "        use_bias=False,\n",
    "    )(conv0_pad)\n",
    "\n",
    "    bn0 = BatchNormalization(epsilon=1.9999999494757503e-05, name=\"bn0\", trainable=False)(conv0)\n",
    "\n",
    "    relu0 = ReLU(name=\"relu0\")(bn0)\n",
    "\n",
    "    pooling0_pad = ZeroPadding2D(padding=tuple([1, 1]))(relu0)\n",
    "\n",
    "    pooling0 = MaxPool2D((3, 3), (2, 2), padding=\"valid\", name=\"pooling0\")(pooling0_pad)\n",
    "\n",
    "    stage1_unit1_bn1 = BatchNormalization(\n",
    "        epsilon=1.9999999494757503e-05, name=\"stage1_unit1_bn1\", trainable=False\n",
    "    )(pooling0)\n",
    "\n",
    "    stage1_unit1_relu1 = ReLU(name=\"stage1_unit1_relu1\")(stage1_unit1_bn1)\n",
    "\n",
    "    stage1_unit1_conv1 = Conv2D(\n",
    "        filters=64,\n",
    "        kernel_size=(1, 1),\n",
    "        name=\"stage1_unit1_conv1\",\n",
    "        strides=[1, 1],\n",
    "        padding=\"VALID\",\n",
    "        use_bias=False,\n",
    "    )(stage1_unit1_relu1)\n",
    "\n",
    "    stage1_unit1_sc = Conv2D(\n",
    "        filters=256,\n",
    "        kernel_size=(1, 1),\n",
    "        name=\"stage1_unit1_sc\",\n",
    "        strides=[1, 1],\n",
    "        padding=\"VALID\",\n",
    "        use_bias=False,\n",
    "    )(stage1_unit1_relu1)\n",
    "\n",
    "    stage1_unit1_bn2 = BatchNormalization(\n",
    "        epsilon=1.9999999494757503e-05, name=\"stage1_unit1_bn2\", trainable=False\n",
    "    )(stage1_unit1_conv1)\n",
    "\n",
    "    stage1_unit1_relu2 = ReLU(name=\"stage1_unit1_relu2\")(stage1_unit1_bn2)\n",
    "\n",
    "    stage1_unit1_conv2_pad = ZeroPadding2D(padding=tuple([1, 1]))(stage1_unit1_relu2)\n",
    "\n",
    "    stage1_unit1_conv2 = Conv2D(\n",
    "        filters=64,\n",
    "        kernel_size=(3, 3),\n",
    "        name=\"stage1_unit1_conv2\",\n",
    "        strides=[1, 1],\n",
    "        padding=\"VALID\",\n",
    "        use_bias=False,\n",
    "    )(stage1_unit1_conv2_pad)\n",
    "\n",
    "    stage1_unit1_bn3 = BatchNormalization(\n",
    "        epsilon=1.9999999494757503e-05, name=\"stage1_unit1_bn3\", trainable=False\n",
    "    )(stage1_unit1_conv2)\n",
    "\n",
    "    stage1_unit1_relu3 = ReLU(name=\"stage1_unit1_relu3\")(stage1_unit1_bn3)\n",
    "\n",
    "    stage1_unit1_conv3 = Conv2D(\n",
    "        filters=256,\n",
    "        kernel_size=(1, 1),\n",
    "        name=\"stage1_unit1_conv3\",\n",
    "        strides=[1, 1],\n",
    "        padding=\"VALID\",\n",
    "        use_bias=False,\n",
    "    )(stage1_unit1_relu3)\n",
    "\n",
    "    plus0_v1 = Add()([stage1_unit1_conv3, stage1_unit1_sc])\n",
    "\n",
    "    stage1_unit2_bn1 = BatchNormalization(\n",
    "        epsilon=1.9999999494757503e-05, name=\"stage1_unit2_bn1\", trainable=False\n",
    "    )(plus0_v1)\n",
    "\n",
    "    stage1_unit2_relu1 = ReLU(name=\"stage1_unit2_relu1\")(stage1_unit2_bn1)\n",
    "\n",
    "    stage1_unit2_conv1 = Conv2D(\n",
    "        filters=64,\n",
    "        kernel_size=(1, 1),\n",
    "        name=\"stage1_unit2_conv1\",\n",
    "        strides=[1, 1],\n",
    "        padding=\"VALID\",\n",
    "        use_bias=False,\n",
    "    )(stage1_unit2_relu1)\n",
    "\n",
    "    stage1_unit2_bn2 = BatchNormalization(\n",
    "        epsilon=1.9999999494757503e-05, name=\"stage1_unit2_bn2\", trainable=False\n",
    "    )(stage1_unit2_conv1)\n",
    "\n",
    "    stage1_unit2_relu2 = ReLU(name=\"stage1_unit2_relu2\")(stage1_unit2_bn2)\n",
    "\n",
    "    stage1_unit2_conv2_pad = ZeroPadding2D(padding=tuple([1, 1]))(stage1_unit2_relu2)\n",
    "\n",
    "    stage1_unit2_conv2 = Conv2D(\n",
    "        filters=64,\n",
    "        kernel_size=(3, 3),\n",
    "        name=\"stage1_unit2_conv2\",\n",
    "        strides=[1, 1],\n",
    "        padding=\"VALID\",\n",
    "        use_bias=False,\n",
    "    )(stage1_unit2_conv2_pad)\n",
    "\n",
    "    stage1_unit2_bn3 = BatchNormalization(\n",
    "        epsilon=1.9999999494757503e-05, name=\"stage1_unit2_bn3\", trainable=False\n",
    "    )(stage1_unit2_conv2)\n",
    "\n",
    "    stage1_unit2_relu3 = ReLU(name=\"stage1_unit2_relu3\")(stage1_unit2_bn3)\n",
    "\n",
    "    stage1_unit2_conv3 = Conv2D(\n",
    "        filters=256,\n",
    "        kernel_size=(1, 1),\n",
    "        name=\"stage1_unit2_conv3\",\n",
    "        strides=[1, 1],\n",
    "        padding=\"VALID\",\n",
    "        use_bias=False,\n",
    "    )(stage1_unit2_relu3)\n",
    "\n",
    "    plus1_v2 = Add()([stage1_unit2_conv3, plus0_v1])\n",
    "\n",
    "    stage1_unit3_bn1 = BatchNormalization(\n",
    "        epsilon=1.9999999494757503e-05, name=\"stage1_unit3_bn1\", trainable=False\n",
    "    )(plus1_v2)\n",
    "\n",
    "    stage1_unit3_relu1 = ReLU(name=\"stage1_unit3_relu1\")(stage1_unit3_bn1)\n",
    "\n",
    "    stage1_unit3_conv1 = Conv2D(\n",
    "        filters=64,\n",
    "        kernel_size=(1, 1),\n",
    "        name=\"stage1_unit3_conv1\",\n",
    "        strides=[1, 1],\n",
    "        padding=\"VALID\",\n",
    "        use_bias=False,\n",
    "    )(stage1_unit3_relu1)\n",
    "\n",
    "    stage1_unit3_bn2 = BatchNormalization(\n",
    "        epsilon=1.9999999494757503e-05, name=\"stage1_unit3_bn2\", trainable=False\n",
    "    )(stage1_unit3_conv1)\n",
    "\n",
    "    stage1_unit3_relu2 = ReLU(name=\"stage1_unit3_relu2\")(stage1_unit3_bn2)\n",
    "\n",
    "    stage1_unit3_conv2_pad = ZeroPadding2D(padding=tuple([1, 1]))(stage1_unit3_relu2)\n",
    "\n",
    "    stage1_unit3_conv2 = Conv2D(\n",
    "        filters=64,\n",
    "        kernel_size=(3, 3),\n",
    "        name=\"stage1_unit3_conv2\",\n",
    "        strides=[1, 1],\n",
    "        padding=\"VALID\",\n",
    "        use_bias=False,\n",
    "    )(stage1_unit3_conv2_pad)\n",
    "\n",
    "    stage1_unit3_bn3 = BatchNormalization(\n",
    "        epsilon=1.9999999494757503e-05, name=\"stage1_unit3_bn3\", trainable=False\n",
    "    )(stage1_unit3_conv2)\n",
    "\n",
    "    stage1_unit3_relu3 = ReLU(name=\"stage1_unit3_relu3\")(stage1_unit3_bn3)\n",
    "\n",
    "    stage1_unit3_conv3 = Conv2D(\n",
    "        filters=256,\n",
    "        kernel_size=(1, 1),\n",
    "        name=\"stage1_unit3_conv3\",\n",
    "        strides=[1, 1],\n",
    "        padding=\"VALID\",\n",
    "        use_bias=False,\n",
    "    )(stage1_unit3_relu3)\n",
    "\n",
    "    plus2 = Add()([stage1_unit3_conv3, plus1_v2])\n",
    "\n",
    "    stage2_unit1_bn1 = BatchNormalization(\n",
    "        epsilon=1.9999999494757503e-05, name=\"stage2_unit1_bn1\", trainable=False\n",
    "    )(plus2)\n",
    "\n",
    "    stage2_unit1_relu1 = ReLU(name=\"stage2_unit1_relu1\")(stage2_unit1_bn1)\n",
    "\n",
    "    stage2_unit1_conv1 = Conv2D(\n",
    "        filters=128,\n",
    "        kernel_size=(1, 1),\n",
    "        name=\"stage2_unit1_conv1\",\n",
    "        strides=[1, 1],\n",
    "        padding=\"VALID\",\n",
    "        use_bias=False,\n",
    "    )(stage2_unit1_relu1)\n",
    "\n",
    "    stage2_unit1_sc = Conv2D(\n",
    "        filters=512,\n",
    "        kernel_size=(1, 1),\n",
    "        name=\"stage2_unit1_sc\",\n",
    "        strides=[2, 2],\n",
    "        padding=\"VALID\",\n",
    "        use_bias=False,\n",
    "    )(stage2_unit1_relu1)\n",
    "\n",
    "    stage2_unit1_bn2 = BatchNormalization(\n",
    "        epsilon=1.9999999494757503e-05, name=\"stage2_unit1_bn2\", trainable=False\n",
    "    )(stage2_unit1_conv1)\n",
    "\n",
    "    stage2_unit1_relu2 = ReLU(name=\"stage2_unit1_relu2\")(stage2_unit1_bn2)\n",
    "\n",
    "    stage2_unit1_conv2_pad = ZeroPadding2D(padding=tuple([1, 1]))(stage2_unit1_relu2)\n",
    "\n",
    "    stage2_unit1_conv2 = Conv2D(\n",
    "        filters=128,\n",
    "        kernel_size=(3, 3),\n",
    "        name=\"stage2_unit1_conv2\",\n",
    "        strides=[2, 2],\n",
    "        padding=\"VALID\",\n",
    "        use_bias=False,\n",
    "    )(stage2_unit1_conv2_pad)\n",
    "\n",
    "    stage2_unit1_bn3 = BatchNormalization(\n",
    "        epsilon=1.9999999494757503e-05, name=\"stage2_unit1_bn3\", trainable=False\n",
    "    )(stage2_unit1_conv2)\n",
    "\n",
    "    stage2_unit1_relu3 = ReLU(name=\"stage2_unit1_relu3\")(stage2_unit1_bn3)\n",
    "\n",
    "    stage2_unit1_conv3 = Conv2D(\n",
    "        filters=512,\n",
    "        kernel_size=(1, 1),\n",
    "        name=\"stage2_unit1_conv3\",\n",
    "        strides=[1, 1],\n",
    "        padding=\"VALID\",\n",
    "        use_bias=False,\n",
    "    )(stage2_unit1_relu3)\n",
    "\n",
    "    plus3 = Add()([stage2_unit1_conv3, stage2_unit1_sc])\n",
    "\n",
    "    stage2_unit2_bn1 = BatchNormalization(\n",
    "        epsilon=1.9999999494757503e-05, name=\"stage2_unit2_bn1\", trainable=False\n",
    "    )(plus3)\n",
    "\n",
    "    stage2_unit2_relu1 = ReLU(name=\"stage2_unit2_relu1\")(stage2_unit2_bn1)\n",
    "\n",
    "    stage2_unit2_conv1 = Conv2D(\n",
    "        filters=128,\n",
    "        kernel_size=(1, 1),\n",
    "        name=\"stage2_unit2_conv1\",\n",
    "        strides=[1, 1],\n",
    "        padding=\"VALID\",\n",
    "        use_bias=False,\n",
    "    )(stage2_unit2_relu1)\n",
    "\n",
    "    stage2_unit2_bn2 = BatchNormalization(\n",
    "        epsilon=1.9999999494757503e-05, name=\"stage2_unit2_bn2\", trainable=False\n",
    "    )(stage2_unit2_conv1)\n",
    "\n",
    "    stage2_unit2_relu2 = ReLU(name=\"stage2_unit2_relu2\")(stage2_unit2_bn2)\n",
    "\n",
    "    stage2_unit2_conv2_pad = ZeroPadding2D(padding=tuple([1, 1]))(stage2_unit2_relu2)\n",
    "\n",
    "    stage2_unit2_conv2 = Conv2D(\n",
    "        filters=128,\n",
    "        kernel_size=(3, 3),\n",
    "        name=\"stage2_unit2_conv2\",\n",
    "        strides=[1, 1],\n",
    "        padding=\"VALID\",\n",
    "        use_bias=False,\n",
    "    )(stage2_unit2_conv2_pad)\n",
    "\n",
    "    stage2_unit2_bn3 = BatchNormalization(\n",
    "        epsilon=1.9999999494757503e-05, name=\"stage2_unit2_bn3\", trainable=False\n",
    "    )(stage2_unit2_conv2)\n",
    "\n",
    "    stage2_unit2_relu3 = ReLU(name=\"stage2_unit2_relu3\")(stage2_unit2_bn3)\n",
    "\n",
    "    stage2_unit2_conv3 = Conv2D(\n",
    "        filters=512,\n",
    "        kernel_size=(1, 1),\n",
    "        name=\"stage2_unit2_conv3\",\n",
    "        strides=[1, 1],\n",
    "        padding=\"VALID\",\n",
    "        use_bias=False,\n",
    "    )(stage2_unit2_relu3)\n",
    "\n",
    "    plus4 = Add()([stage2_unit2_conv3, plus3])\n",
    "\n",
    "    stage2_unit3_bn1 = BatchNormalization(\n",
    "        epsilon=1.9999999494757503e-05, name=\"stage2_unit3_bn1\", trainable=False\n",
    "    )(plus4)\n",
    "\n",
    "    stage2_unit3_relu1 = ReLU(name=\"stage2_unit3_relu1\")(stage2_unit3_bn1)\n",
    "\n",
    "    stage2_unit3_conv1 = Conv2D(\n",
    "        filters=128,\n",
    "        kernel_size=(1, 1),\n",
    "        name=\"stage2_unit3_conv1\",\n",
    "        strides=[1, 1],\n",
    "        padding=\"VALID\",\n",
    "        use_bias=False,\n",
    "    )(stage2_unit3_relu1)\n",
    "\n",
    "    stage2_unit3_bn2 = BatchNormalization(\n",
    "        epsilon=1.9999999494757503e-05, name=\"stage2_unit3_bn2\", trainable=False\n",
    "    )(stage2_unit3_conv1)\n",
    "\n",
    "    stage2_unit3_relu2 = ReLU(name=\"stage2_unit3_relu2\")(stage2_unit3_bn2)\n",
    "\n",
    "    stage2_unit3_conv2_pad = ZeroPadding2D(padding=tuple([1, 1]))(stage2_unit3_relu2)\n",
    "\n",
    "    stage2_unit3_conv2 = Conv2D(\n",
    "        filters=128,\n",
    "        kernel_size=(3, 3),\n",
    "        name=\"stage2_unit3_conv2\",\n",
    "        strides=[1, 1],\n",
    "        padding=\"VALID\",\n",
    "        use_bias=False,\n",
    "    )(stage2_unit3_conv2_pad)\n",
    "\n",
    "    stage2_unit3_bn3 = BatchNormalization(\n",
    "        epsilon=1.9999999494757503e-05, name=\"stage2_unit3_bn3\", trainable=False\n",
    "    )(stage2_unit3_conv2)\n",
    "\n",
    "    stage2_unit3_relu3 = ReLU(name=\"stage2_unit3_relu3\")(stage2_unit3_bn3)\n",
    "\n",
    "    stage2_unit3_conv3 = Conv2D(\n",
    "        filters=512,\n",
    "        kernel_size=(1, 1),\n",
    "        name=\"stage2_unit3_conv3\",\n",
    "        strides=[1, 1],\n",
    "        padding=\"VALID\",\n",
    "        use_bias=False,\n",
    "    )(stage2_unit3_relu3)\n",
    "\n",
    "    plus5 = Add()([stage2_unit3_conv3, plus4])\n",
    "\n",
    "    stage2_unit4_bn1 = BatchNormalization(\n",
    "        epsilon=1.9999999494757503e-05, name=\"stage2_unit4_bn1\", trainable=False\n",
    "    )(plus5)\n",
    "\n",
    "    stage2_unit4_relu1 = ReLU(name=\"stage2_unit4_relu1\")(stage2_unit4_bn1)\n",
    "\n",
    "    stage2_unit4_conv1 = Conv2D(\n",
    "        filters=128,\n",
    "        kernel_size=(1, 1),\n",
    "        name=\"stage2_unit4_conv1\",\n",
    "        strides=[1, 1],\n",
    "        padding=\"VALID\",\n",
    "        use_bias=False,\n",
    "    )(stage2_unit4_relu1)\n",
    "\n",
    "    stage2_unit4_bn2 = BatchNormalization(\n",
    "        epsilon=1.9999999494757503e-05, name=\"stage2_unit4_bn2\", trainable=False\n",
    "    )(stage2_unit4_conv1)\n",
    "\n",
    "    stage2_unit4_relu2 = ReLU(name=\"stage2_unit4_relu2\")(stage2_unit4_bn2)\n",
    "\n",
    "    stage2_unit4_conv2_pad = ZeroPadding2D(padding=tuple([1, 1]))(stage2_unit4_relu2)\n",
    "\n",
    "    stage2_unit4_conv2 = Conv2D(\n",
    "        filters=128,\n",
    "        kernel_size=(3, 3),\n",
    "        name=\"stage2_unit4_conv2\",\n",
    "        strides=[1, 1],\n",
    "        padding=\"VALID\",\n",
    "        use_bias=False,\n",
    "    )(stage2_unit4_conv2_pad)\n",
    "\n",
    "    stage2_unit4_bn3 = BatchNormalization(\n",
    "        epsilon=1.9999999494757503e-05, name=\"stage2_unit4_bn3\", trainable=False\n",
    "    )(stage2_unit4_conv2)\n",
    "\n",
    "    stage2_unit4_relu3 = ReLU(name=\"stage2_unit4_relu3\")(stage2_unit4_bn3)\n",
    "\n",
    "    stage2_unit4_conv3 = Conv2D(\n",
    "        filters=512,\n",
    "        kernel_size=(1, 1),\n",
    "        name=\"stage2_unit4_conv3\",\n",
    "        strides=[1, 1],\n",
    "        padding=\"VALID\",\n",
    "        use_bias=False,\n",
    "    )(stage2_unit4_relu3)\n",
    "\n",
    "    plus6 = Add()([stage2_unit4_conv3, plus5])\n",
    "\n",
    "    stage3_unit1_bn1 = BatchNormalization(\n",
    "        epsilon=1.9999999494757503e-05, name=\"stage3_unit1_bn1\", trainable=False\n",
    "    )(plus6)\n",
    "\n",
    "    stage3_unit1_relu1 = ReLU(name=\"stage3_unit1_relu1\")(stage3_unit1_bn1)\n",
    "\n",
    "    stage3_unit1_conv1 = Conv2D(\n",
    "        filters=256,\n",
    "        kernel_size=(1, 1),\n",
    "        name=\"stage3_unit1_conv1\",\n",
    "        strides=[1, 1],\n",
    "        padding=\"VALID\",\n",
    "        use_bias=False,\n",
    "    )(stage3_unit1_relu1)\n",
    "\n",
    "    stage3_unit1_sc = Conv2D(\n",
    "        filters=1024,\n",
    "        kernel_size=(1, 1),\n",
    "        name=\"stage3_unit1_sc\",\n",
    "        strides=[2, 2],\n",
    "        padding=\"VALID\",\n",
    "        use_bias=False,\n",
    "    )(stage3_unit1_relu1)\n",
    "\n",
    "    stage3_unit1_bn2 = BatchNormalization(\n",
    "        epsilon=1.9999999494757503e-05, name=\"stage3_unit1_bn2\", trainable=False\n",
    "    )(stage3_unit1_conv1)\n",
    "\n",
    "    stage3_unit1_relu2 = ReLU(name=\"stage3_unit1_relu2\")(stage3_unit1_bn2)\n",
    "\n",
    "    stage3_unit1_conv2_pad = ZeroPadding2D(padding=tuple([1, 1]))(stage3_unit1_relu2)\n",
    "\n",
    "    stage3_unit1_conv2 = Conv2D(\n",
    "        filters=256,\n",
    "        kernel_size=(3, 3),\n",
    "        name=\"stage3_unit1_conv2\",\n",
    "        strides=[2, 2],\n",
    "        padding=\"VALID\",\n",
    "        use_bias=False,\n",
    "    )(stage3_unit1_conv2_pad)\n",
    "\n",
    "    ssh_m1_red_conv = Conv2D(\n",
    "        filters=256,\n",
    "        kernel_size=(1, 1),\n",
    "        name=\"ssh_m1_red_conv\",\n",
    "        strides=[1, 1],\n",
    "        padding=\"VALID\",\n",
    "        use_bias=True,\n",
    "    )(stage3_unit1_relu2)\n",
    "\n",
    "    stage3_unit1_bn3 = BatchNormalization(\n",
    "        epsilon=1.9999999494757503e-05, name=\"stage3_unit1_bn3\", trainable=False\n",
    "    )(stage3_unit1_conv2)\n",
    "\n",
    "    ssh_m1_red_conv_bn = BatchNormalization(\n",
    "        epsilon=1.9999999494757503e-05, name=\"ssh_m1_red_conv_bn\", trainable=False\n",
    "    )(ssh_m1_red_conv)\n",
    "\n",
    "    stage3_unit1_relu3 = ReLU(name=\"stage3_unit1_relu3\")(stage3_unit1_bn3)\n",
    "\n",
    "    ssh_m1_red_conv_relu = ReLU(name=\"ssh_m1_red_conv_relu\")(ssh_m1_red_conv_bn)\n",
    "\n",
    "    stage3_unit1_conv3 = Conv2D(\n",
    "        filters=1024,\n",
    "        kernel_size=(1, 1),\n",
    "        name=\"stage3_unit1_conv3\",\n",
    "        strides=[1, 1],\n",
    "        padding=\"VALID\",\n",
    "        use_bias=False,\n",
    "    )(stage3_unit1_relu3)\n",
    "\n",
    "    plus7 = Add()([stage3_unit1_conv3, stage3_unit1_sc])\n",
    "\n",
    "    stage3_unit2_bn1 = BatchNormalization(\n",
    "        epsilon=1.9999999494757503e-05, name=\"stage3_unit2_bn1\", trainable=False\n",
    "    )(plus7)\n",
    "\n",
    "    stage3_unit2_relu1 = ReLU(name=\"stage3_unit2_relu1\")(stage3_unit2_bn1)\n",
    "\n",
    "    stage3_unit2_conv1 = Conv2D(\n",
    "        filters=256,\n",
    "        kernel_size=(1, 1),\n",
    "        name=\"stage3_unit2_conv1\",\n",
    "        strides=[1, 1],\n",
    "        padding=\"VALID\",\n",
    "        use_bias=False,\n",
    "    )(stage3_unit2_relu1)\n",
    "\n",
    "    stage3_unit2_bn2 = BatchNormalization(\n",
    "        epsilon=1.9999999494757503e-05, name=\"stage3_unit2_bn2\", trainable=False\n",
    "    )(stage3_unit2_conv1)\n",
    "\n",
    "    stage3_unit2_relu2 = ReLU(name=\"stage3_unit2_relu2\")(stage3_unit2_bn2)\n",
    "\n",
    "    stage3_unit2_conv2_pad = ZeroPadding2D(padding=tuple([1, 1]))(stage3_unit2_relu2)\n",
    "\n",
    "    stage3_unit2_conv2 = Conv2D(\n",
    "        filters=256,\n",
    "        kernel_size=(3, 3),\n",
    "        name=\"stage3_unit2_conv2\",\n",
    "        strides=[1, 1],\n",
    "        padding=\"VALID\",\n",
    "        use_bias=False,\n",
    "    )(stage3_unit2_conv2_pad)\n",
    "\n",
    "    stage3_unit2_bn3 = BatchNormalization(\n",
    "        epsilon=1.9999999494757503e-05, name=\"stage3_unit2_bn3\", trainable=False\n",
    "    )(stage3_unit2_conv2)\n",
    "\n",
    "    stage3_unit2_relu3 = ReLU(name=\"stage3_unit2_relu3\")(stage3_unit2_bn3)\n",
    "\n",
    "    stage3_unit2_conv3 = Conv2D(\n",
    "        filters=1024,\n",
    "        kernel_size=(1, 1),\n",
    "        name=\"stage3_unit2_conv3\",\n",
    "        strides=[1, 1],\n",
    "        padding=\"VALID\",\n",
    "        use_bias=False,\n",
    "    )(stage3_unit2_relu3)\n",
    "\n",
    "    plus8 = Add()([stage3_unit2_conv3, plus7])\n",
    "\n",
    "    stage3_unit3_bn1 = BatchNormalization(\n",
    "        epsilon=1.9999999494757503e-05, name=\"stage3_unit3_bn1\", trainable=False\n",
    "    )(plus8)\n",
    "\n",
    "    stage3_unit3_relu1 = ReLU(name=\"stage3_unit3_relu1\")(stage3_unit3_bn1)\n",
    "\n",
    "    stage3_unit3_conv1 = Conv2D(\n",
    "        filters=256,\n",
    "        kernel_size=(1, 1),\n",
    "        name=\"stage3_unit3_conv1\",\n",
    "        strides=[1, 1],\n",
    "        padding=\"VALID\",\n",
    "        use_bias=False,\n",
    "    )(stage3_unit3_relu1)\n",
    "\n",
    "    stage3_unit3_bn2 = BatchNormalization(\n",
    "        epsilon=1.9999999494757503e-05, name=\"stage3_unit3_bn2\", trainable=False\n",
    "    )(stage3_unit3_conv1)\n",
    "\n",
    "    stage3_unit3_relu2 = ReLU(name=\"stage3_unit3_relu2\")(stage3_unit3_bn2)\n",
    "\n",
    "    stage3_unit3_conv2_pad = ZeroPadding2D(padding=tuple([1, 1]))(stage3_unit3_relu2)\n",
    "\n",
    "    stage3_unit3_conv2 = Conv2D(\n",
    "        filters=256,\n",
    "        kernel_size=(3, 3),\n",
    "        name=\"stage3_unit3_conv2\",\n",
    "        strides=[1, 1],\n",
    "        padding=\"VALID\",\n",
    "        use_bias=False,\n",
    "    )(stage3_unit3_conv2_pad)\n",
    "\n",
    "    stage3_unit3_bn3 = BatchNormalization(\n",
    "        epsilon=1.9999999494757503e-05, name=\"stage3_unit3_bn3\", trainable=False\n",
    "    )(stage3_unit3_conv2)\n",
    "\n",
    "    stage3_unit3_relu3 = ReLU(name=\"stage3_unit3_relu3\")(stage3_unit3_bn3)\n",
    "\n",
    "    stage3_unit3_conv3 = Conv2D(\n",
    "        filters=1024,\n",
    "        kernel_size=(1, 1),\n",
    "        name=\"stage3_unit3_conv3\",\n",
    "        strides=[1, 1],\n",
    "        padding=\"VALID\",\n",
    "        use_bias=False,\n",
    "    )(stage3_unit3_relu3)\n",
    "\n",
    "    plus9 = Add()([stage3_unit3_conv3, plus8])\n",
    "\n",
    "    stage3_unit4_bn1 = BatchNormalization(\n",
    "        epsilon=1.9999999494757503e-05, name=\"stage3_unit4_bn1\", trainable=False\n",
    "    )(plus9)\n",
    "\n",
    "    stage3_unit4_relu1 = ReLU(name=\"stage3_unit4_relu1\")(stage3_unit4_bn1)\n",
    "\n",
    "    stage3_unit4_conv1 = Conv2D(\n",
    "        filters=256,\n",
    "        kernel_size=(1, 1),\n",
    "        name=\"stage3_unit4_conv1\",\n",
    "        strides=[1, 1],\n",
    "        padding=\"VALID\",\n",
    "        use_bias=False,\n",
    "    )(stage3_unit4_relu1)\n",
    "\n",
    "    stage3_unit4_bn2 = BatchNormalization(\n",
    "        epsilon=1.9999999494757503e-05, name=\"stage3_unit4_bn2\", trainable=False\n",
    "    )(stage3_unit4_conv1)\n",
    "\n",
    "    stage3_unit4_relu2 = ReLU(name=\"stage3_unit4_relu2\")(stage3_unit4_bn2)\n",
    "\n",
    "    stage3_unit4_conv2_pad = ZeroPadding2D(padding=tuple([1, 1]))(stage3_unit4_relu2)\n",
    "\n",
    "    stage3_unit4_conv2 = Conv2D(\n",
    "        filters=256,\n",
    "        kernel_size=(3, 3),\n",
    "        name=\"stage3_unit4_conv2\",\n",
    "        strides=[1, 1],\n",
    "        padding=\"VALID\",\n",
    "        use_bias=False,\n",
    "    )(stage3_unit4_conv2_pad)\n",
    "\n",
    "    stage3_unit4_bn3 = BatchNormalization(\n",
    "        epsilon=1.9999999494757503e-05, name=\"stage3_unit4_bn3\", trainable=False\n",
    "    )(stage3_unit4_conv2)\n",
    "\n",
    "    stage3_unit4_relu3 = ReLU(name=\"stage3_unit4_relu3\")(stage3_unit4_bn3)\n",
    "\n",
    "    stage3_unit4_conv3 = Conv2D(\n",
    "        filters=1024,\n",
    "        kernel_size=(1, 1),\n",
    "        name=\"stage3_unit4_conv3\",\n",
    "        strides=[1, 1],\n",
    "        padding=\"VALID\",\n",
    "        use_bias=False,\n",
    "    )(stage3_unit4_relu3)\n",
    "\n",
    "    plus10 = Add()([stage3_unit4_conv3, plus9])\n",
    "\n",
    "    stage3_unit5_bn1 = BatchNormalization(\n",
    "        epsilon=1.9999999494757503e-05, name=\"stage3_unit5_bn1\", trainable=False\n",
    "    )(plus10)\n",
    "\n",
    "    stage3_unit5_relu1 = ReLU(name=\"stage3_unit5_relu1\")(stage3_unit5_bn1)\n",
    "\n",
    "    stage3_unit5_conv1 = Conv2D(\n",
    "        filters=256,\n",
    "        kernel_size=(1, 1),\n",
    "        name=\"stage3_unit5_conv1\",\n",
    "        strides=[1, 1],\n",
    "        padding=\"VALID\",\n",
    "        use_bias=False,\n",
    "    )(stage3_unit5_relu1)\n",
    "\n",
    "    stage3_unit5_bn2 = BatchNormalization(\n",
    "        epsilon=1.9999999494757503e-05, name=\"stage3_unit5_bn2\", trainable=False\n",
    "    )(stage3_unit5_conv1)\n",
    "\n",
    "    stage3_unit5_relu2 = ReLU(name=\"stage3_unit5_relu2\")(stage3_unit5_bn2)\n",
    "\n",
    "    stage3_unit5_conv2_pad = ZeroPadding2D(padding=tuple([1, 1]))(stage3_unit5_relu2)\n",
    "\n",
    "    stage3_unit5_conv2 = Conv2D(\n",
    "        filters=256,\n",
    "        kernel_size=(3, 3),\n",
    "        name=\"stage3_unit5_conv2\",\n",
    "        strides=[1, 1],\n",
    "        padding=\"VALID\",\n",
    "        use_bias=False,\n",
    "    )(stage3_unit5_conv2_pad)\n",
    "\n",
    "    stage3_unit5_bn3 = BatchNormalization(\n",
    "        epsilon=1.9999999494757503e-05, name=\"stage3_unit5_bn3\", trainable=False\n",
    "    )(stage3_unit5_conv2)\n",
    "\n",
    "    stage3_unit5_relu3 = ReLU(name=\"stage3_unit5_relu3\")(stage3_unit5_bn3)\n",
    "\n",
    "    stage3_unit5_conv3 = Conv2D(\n",
    "        filters=1024,\n",
    "        kernel_size=(1, 1),\n",
    "        name=\"stage3_unit5_conv3\",\n",
    "        strides=[1, 1],\n",
    "        padding=\"VALID\",\n",
    "        use_bias=False,\n",
    "    )(stage3_unit5_relu3)\n",
    "\n",
    "    plus11 = Add()([stage3_unit5_conv3, plus10])\n",
    "\n",
    "    stage3_unit6_bn1 = BatchNormalization(\n",
    "        epsilon=1.9999999494757503e-05, name=\"stage3_unit6_bn1\", trainable=False\n",
    "    )(plus11)\n",
    "\n",
    "    stage3_unit6_relu1 = ReLU(name=\"stage3_unit6_relu1\")(stage3_unit6_bn1)\n",
    "\n",
    "    stage3_unit6_conv1 = Conv2D(\n",
    "        filters=256,\n",
    "        kernel_size=(1, 1),\n",
    "        name=\"stage3_unit6_conv1\",\n",
    "        strides=[1, 1],\n",
    "        padding=\"VALID\",\n",
    "        use_bias=False,\n",
    "    )(stage3_unit6_relu1)\n",
    "\n",
    "    stage3_unit6_bn2 = BatchNormalization(\n",
    "        epsilon=1.9999999494757503e-05, name=\"stage3_unit6_bn2\", trainable=False\n",
    "    )(stage3_unit6_conv1)\n",
    "\n",
    "    stage3_unit6_relu2 = ReLU(name=\"stage3_unit6_relu2\")(stage3_unit6_bn2)\n",
    "\n",
    "    stage3_unit6_conv2_pad = ZeroPadding2D(padding=tuple([1, 1]))(stage3_unit6_relu2)\n",
    "\n",
    "    stage3_unit6_conv2 = Conv2D(\n",
    "        filters=256,\n",
    "        kernel_size=(3, 3),\n",
    "        name=\"stage3_unit6_conv2\",\n",
    "        strides=[1, 1],\n",
    "        padding=\"VALID\",\n",
    "        use_bias=False,\n",
    "    )(stage3_unit6_conv2_pad)\n",
    "\n",
    "    stage3_unit6_bn3 = BatchNormalization(\n",
    "        epsilon=1.9999999494757503e-05, name=\"stage3_unit6_bn3\", trainable=False\n",
    "    )(stage3_unit6_conv2)\n",
    "\n",
    "    stage3_unit6_relu3 = ReLU(name=\"stage3_unit6_relu3\")(stage3_unit6_bn3)\n",
    "\n",
    "    stage3_unit6_conv3 = Conv2D(\n",
    "        filters=1024,\n",
    "        kernel_size=(1, 1),\n",
    "        name=\"stage3_unit6_conv3\",\n",
    "        strides=[1, 1],\n",
    "        padding=\"VALID\",\n",
    "        use_bias=False,\n",
    "    )(stage3_unit6_relu3)\n",
    "\n",
    "    plus12 = Add()([stage3_unit6_conv3, plus11])\n",
    "\n",
    "    stage4_unit1_bn1 = BatchNormalization(\n",
    "        epsilon=1.9999999494757503e-05, name=\"stage4_unit1_bn1\", trainable=False\n",
    "    )(plus12)\n",
    "\n",
    "    stage4_unit1_relu1 = ReLU(name=\"stage4_unit1_relu1\")(stage4_unit1_bn1)\n",
    "\n",
    "    stage4_unit1_conv1 = Conv2D(\n",
    "        filters=512,\n",
    "        kernel_size=(1, 1),\n",
    "        name=\"stage4_unit1_conv1\",\n",
    "        strides=[1, 1],\n",
    "        padding=\"VALID\",\n",
    "        use_bias=False,\n",
    "    )(stage4_unit1_relu1)\n",
    "\n",
    "    stage4_unit1_sc = Conv2D(\n",
    "        filters=2048,\n",
    "        kernel_size=(1, 1),\n",
    "        name=\"stage4_unit1_sc\",\n",
    "        strides=[2, 2],\n",
    "        padding=\"VALID\",\n",
    "        use_bias=False,\n",
    "    )(stage4_unit1_relu1)\n",
    "\n",
    "    stage4_unit1_bn2 = BatchNormalization(\n",
    "        epsilon=1.9999999494757503e-05, name=\"stage4_unit1_bn2\", trainable=False\n",
    "    )(stage4_unit1_conv1)\n",
    "\n",
    "    stage4_unit1_relu2 = ReLU(name=\"stage4_unit1_relu2\")(stage4_unit1_bn2)\n",
    "\n",
    "    stage4_unit1_conv2_pad = ZeroPadding2D(padding=tuple([1, 1]))(stage4_unit1_relu2)\n",
    "\n",
    "    stage4_unit1_conv2 = Conv2D(\n",
    "        filters=512,\n",
    "        kernel_size=(3, 3),\n",
    "        name=\"stage4_unit1_conv2\",\n",
    "        strides=[2, 2],\n",
    "        padding=\"VALID\",\n",
    "        use_bias=False,\n",
    "    )(stage4_unit1_conv2_pad)\n",
    "\n",
    "    ssh_c2_lateral = Conv2D(\n",
    "        filters=256,\n",
    "        kernel_size=(1, 1),\n",
    "        name=\"ssh_c2_lateral\",\n",
    "        strides=[1, 1],\n",
    "        padding=\"VALID\",\n",
    "        use_bias=True,\n",
    "    )(stage4_unit1_relu2)\n",
    "\n",
    "    stage4_unit1_bn3 = BatchNormalization(\n",
    "        epsilon=1.9999999494757503e-05, name=\"stage4_unit1_bn3\", trainable=False\n",
    "    )(stage4_unit1_conv2)\n",
    "\n",
    "    ssh_c2_lateral_bn = BatchNormalization(\n",
    "        epsilon=1.9999999494757503e-05, name=\"ssh_c2_lateral_bn\", trainable=False\n",
    "    )(ssh_c2_lateral)\n",
    "\n",
    "    stage4_unit1_relu3 = ReLU(name=\"stage4_unit1_relu3\")(stage4_unit1_bn3)\n",
    "\n",
    "    ssh_c2_lateral_relu = ReLU(name=\"ssh_c2_lateral_relu\")(ssh_c2_lateral_bn)\n",
    "\n",
    "    stage4_unit1_conv3 = Conv2D(\n",
    "        filters=2048,\n",
    "        kernel_size=(1, 1),\n",
    "        name=\"stage4_unit1_conv3\",\n",
    "        strides=[1, 1],\n",
    "        padding=\"VALID\",\n",
    "        use_bias=False,\n",
    "    )(stage4_unit1_relu3)\n",
    "\n",
    "    plus13 = Add()([stage4_unit1_conv3, stage4_unit1_sc])\n",
    "\n",
    "    stage4_unit2_bn1 = BatchNormalization(\n",
    "        epsilon=1.9999999494757503e-05, name=\"stage4_unit2_bn1\", trainable=False\n",
    "    )(plus13)\n",
    "\n",
    "    stage4_unit2_relu1 = ReLU(name=\"stage4_unit2_relu1\")(stage4_unit2_bn1)\n",
    "\n",
    "    stage4_unit2_conv1 = Conv2D(\n",
    "        filters=512,\n",
    "        kernel_size=(1, 1),\n",
    "        name=\"stage4_unit2_conv1\",\n",
    "        strides=[1, 1],\n",
    "        padding=\"VALID\",\n",
    "        use_bias=False,\n",
    "    )(stage4_unit2_relu1)\n",
    "\n",
    "    stage4_unit2_bn2 = BatchNormalization(\n",
    "        epsilon=1.9999999494757503e-05, name=\"stage4_unit2_bn2\", trainable=False\n",
    "    )(stage4_unit2_conv1)\n",
    "\n",
    "    stage4_unit2_relu2 = ReLU(name=\"stage4_unit2_relu2\")(stage4_unit2_bn2)\n",
    "\n",
    "    stage4_unit2_conv2_pad = ZeroPadding2D(padding=tuple([1, 1]))(stage4_unit2_relu2)\n",
    "\n",
    "    stage4_unit2_conv2 = Conv2D(\n",
    "        filters=512,\n",
    "        kernel_size=(3, 3),\n",
    "        name=\"stage4_unit2_conv2\",\n",
    "        strides=[1, 1],\n",
    "        padding=\"VALID\",\n",
    "        use_bias=False,\n",
    "    )(stage4_unit2_conv2_pad)\n",
    "\n",
    "    stage4_unit2_bn3 = BatchNormalization(\n",
    "        epsilon=1.9999999494757503e-05, name=\"stage4_unit2_bn3\", trainable=False\n",
    "    )(stage4_unit2_conv2)\n",
    "\n",
    "    stage4_unit2_relu3 = ReLU(name=\"stage4_unit2_relu3\")(stage4_unit2_bn3)\n",
    "\n",
    "    stage4_unit2_conv3 = Conv2D(\n",
    "        filters=2048,\n",
    "        kernel_size=(1, 1),\n",
    "        name=\"stage4_unit2_conv3\",\n",
    "        strides=[1, 1],\n",
    "        padding=\"VALID\",\n",
    "        use_bias=False,\n",
    "    )(stage4_unit2_relu3)\n",
    "\n",
    "    plus14 = Add()([stage4_unit2_conv3, plus13])\n",
    "\n",
    "    stage4_unit3_bn1 = BatchNormalization(\n",
    "        epsilon=1.9999999494757503e-05, name=\"stage4_unit3_bn1\", trainable=False\n",
    "    )(plus14)\n",
    "\n",
    "    stage4_unit3_relu1 = ReLU(name=\"stage4_unit3_relu1\")(stage4_unit3_bn1)\n",
    "\n",
    "    stage4_unit3_conv1 = Conv2D(\n",
    "        filters=512,\n",
    "        kernel_size=(1, 1),\n",
    "        name=\"stage4_unit3_conv1\",\n",
    "        strides=[1, 1],\n",
    "        padding=\"VALID\",\n",
    "        use_bias=False,\n",
    "    )(stage4_unit3_relu1)\n",
    "\n",
    "    stage4_unit3_bn2 = BatchNormalization(\n",
    "        epsilon=1.9999999494757503e-05, name=\"stage4_unit3_bn2\", trainable=False\n",
    "    )(stage4_unit3_conv1)\n",
    "\n",
    "    stage4_unit3_relu2 = ReLU(name=\"stage4_unit3_relu2\")(stage4_unit3_bn2)\n",
    "\n",
    "    stage4_unit3_conv2_pad = ZeroPadding2D(padding=tuple([1, 1]))(stage4_unit3_relu2)\n",
    "\n",
    "    stage4_unit3_conv2 = Conv2D(\n",
    "        filters=512,\n",
    "        kernel_size=(3, 3),\n",
    "        name=\"stage4_unit3_conv2\",\n",
    "        strides=[1, 1],\n",
    "        padding=\"VALID\",\n",
    "        use_bias=False,\n",
    "    )(stage4_unit3_conv2_pad)\n",
    "\n",
    "    stage4_unit3_bn3 = BatchNormalization(\n",
    "        epsilon=1.9999999494757503e-05, name=\"stage4_unit3_bn3\", trainable=False\n",
    "    )(stage4_unit3_conv2)\n",
    "\n",
    "    stage4_unit3_relu3 = ReLU(name=\"stage4_unit3_relu3\")(stage4_unit3_bn3)\n",
    "\n",
    "    stage4_unit3_conv3 = Conv2D(\n",
    "        filters=2048,\n",
    "        kernel_size=(1, 1),\n",
    "        name=\"stage4_unit3_conv3\",\n",
    "        strides=[1, 1],\n",
    "        padding=\"VALID\",\n",
    "        use_bias=False,\n",
    "    )(stage4_unit3_relu3)\n",
    "\n",
    "    plus15 = Add()([stage4_unit3_conv3, plus14])\n",
    "\n",
    "    bn1 = BatchNormalization(epsilon=1.9999999494757503e-05, name=\"bn1\", trainable=False)(plus15)\n",
    "\n",
    "    relu1 = ReLU(name=\"relu1\")(bn1)\n",
    "\n",
    "    ssh_c3_lateral = Conv2D(\n",
    "        filters=256,\n",
    "        kernel_size=(1, 1),\n",
    "        name=\"ssh_c3_lateral\",\n",
    "        strides=[1, 1],\n",
    "        padding=\"VALID\",\n",
    "        use_bias=True,\n",
    "    )(relu1)\n",
    "\n",
    "    ssh_c3_lateral_bn = BatchNormalization(\n",
    "        epsilon=1.9999999494757503e-05, name=\"ssh_c3_lateral_bn\", trainable=False\n",
    "    )(ssh_c3_lateral)\n",
    "\n",
    "    ssh_c3_lateral_relu = ReLU(name=\"ssh_c3_lateral_relu\")(ssh_c3_lateral_bn)\n",
    "\n",
    "    ssh_m3_det_conv1_pad = ZeroPadding2D(padding=tuple([1, 1]))(ssh_c3_lateral_relu)\n",
    "\n",
    "    ssh_m3_det_conv1 = Conv2D(\n",
    "        filters=256,\n",
    "        kernel_size=(3, 3),\n",
    "        name=\"ssh_m3_det_conv1\",\n",
    "        strides=[1, 1],\n",
    "        padding=\"VALID\",\n",
    "        use_bias=True,\n",
    "    )(ssh_m3_det_conv1_pad)\n",
    "\n",
    "    ssh_m3_det_context_conv1_pad = ZeroPadding2D(padding=tuple([1, 1]))(ssh_c3_lateral_relu)\n",
    "\n",
    "    ssh_m3_det_context_conv1 = Conv2D(\n",
    "        filters=128,\n",
    "        kernel_size=(3, 3),\n",
    "        name=\"ssh_m3_det_context_conv1\",\n",
    "        strides=[1, 1],\n",
    "        padding=\"VALID\",\n",
    "        use_bias=True,\n",
    "    )(ssh_m3_det_context_conv1_pad)\n",
    "\n",
    "    ssh_c3_up = UpSampling2D(size=(2, 2), interpolation=\"nearest\", name=\"ssh_c3_up\")(\n",
    "        ssh_c3_lateral_relu\n",
    "    )\n",
    "\n",
    "    ssh_m3_det_conv1_bn = BatchNormalization(\n",
    "        epsilon=1.9999999494757503e-05, name=\"ssh_m3_det_conv1_bn\", trainable=False\n",
    "    )(ssh_m3_det_conv1)\n",
    "\n",
    "    ssh_m3_det_context_conv1_bn = BatchNormalization(\n",
    "        epsilon=1.9999999494757503e-05, name=\"ssh_m3_det_context_conv1_bn\", trainable=False\n",
    "    )(ssh_m3_det_context_conv1)\n",
    "\n",
    "    x1_shape = tf.shape(ssh_c3_up)\n",
    "    x2_shape = tf.shape(ssh_c2_lateral_relu)\n",
    "    offsets = [0, (x1_shape[1] - x2_shape[1]) // 2, (x1_shape[2] - x2_shape[2]) // 2, 0]\n",
    "    size = [-1, x2_shape[1], x2_shape[2], -1]\n",
    "    crop0 = tf.slice(ssh_c3_up, offsets, size, \"crop0\")\n",
    "\n",
    "    ssh_m3_det_context_conv1_relu = ReLU(name=\"ssh_m3_det_context_conv1_relu\")(\n",
    "        ssh_m3_det_context_conv1_bn\n",
    "    )\n",
    "\n",
    "    plus0_v2 = Add()([ssh_c2_lateral_relu, crop0])\n",
    "\n",
    "    ssh_m3_det_context_conv2_pad = ZeroPadding2D(padding=tuple([1, 1]))(\n",
    "        ssh_m3_det_context_conv1_relu\n",
    "    )\n",
    "\n",
    "    ssh_m3_det_context_conv2 = Conv2D(\n",
    "        filters=128,\n",
    "        kernel_size=(3, 3),\n",
    "        name=\"ssh_m3_det_context_conv2\",\n",
    "        strides=[1, 1],\n",
    "        padding=\"VALID\",\n",
    "        use_bias=True,\n",
    "    )(ssh_m3_det_context_conv2_pad)\n",
    "\n",
    "    ssh_m3_det_context_conv3_1_pad = ZeroPadding2D(padding=tuple([1, 1]))(\n",
    "        ssh_m3_det_context_conv1_relu\n",
    "    )\n",
    "\n",
    "    ssh_m3_det_context_conv3_1 = Conv2D(\n",
    "        filters=128,\n",
    "        kernel_size=(3, 3),\n",
    "        name=\"ssh_m3_det_context_conv3_1\",\n",
    "        strides=[1, 1],\n",
    "        padding=\"VALID\",\n",
    "        use_bias=True,\n",
    "    )(ssh_m3_det_context_conv3_1_pad)\n",
    "\n",
    "    ssh_c2_aggr_pad = ZeroPadding2D(padding=tuple([1, 1]))(plus0_v2)\n",
    "\n",
    "    ssh_c2_aggr = Conv2D(\n",
    "        filters=256,\n",
    "        kernel_size=(3, 3),\n",
    "        name=\"ssh_c2_aggr\",\n",
    "        strides=[1, 1],\n",
    "        padding=\"VALID\",\n",
    "        use_bias=True,\n",
    "    )(ssh_c2_aggr_pad)\n",
    "\n",
    "    ssh_m3_det_context_conv2_bn = BatchNormalization(\n",
    "        epsilon=1.9999999494757503e-05, name=\"ssh_m3_det_context_conv2_bn\", trainable=False\n",
    "    )(ssh_m3_det_context_conv2)\n",
    "\n",
    "    ssh_m3_det_context_conv3_1_bn = BatchNormalization(\n",
    "        epsilon=1.9999999494757503e-05, name=\"ssh_m3_det_context_conv3_1_bn\", trainable=False\n",
    "    )(ssh_m3_det_context_conv3_1)\n",
    "\n",
    "    ssh_c2_aggr_bn = BatchNormalization(\n",
    "        epsilon=1.9999999494757503e-05, name=\"ssh_c2_aggr_bn\", trainable=False\n",
    "    )(ssh_c2_aggr)\n",
    "\n",
    "    ssh_m3_det_context_conv3_1_relu = ReLU(name=\"ssh_m3_det_context_conv3_1_relu\")(\n",
    "        ssh_m3_det_context_conv3_1_bn\n",
    "    )\n",
    "\n",
    "    ssh_c2_aggr_relu = ReLU(name=\"ssh_c2_aggr_relu\")(ssh_c2_aggr_bn)\n",
    "\n",
    "    ssh_m3_det_context_conv3_2_pad = ZeroPadding2D(padding=tuple([1, 1]))(\n",
    "        ssh_m3_det_context_conv3_1_relu\n",
    "    )\n",
    "\n",
    "    ssh_m3_det_context_conv3_2 = Conv2D(\n",
    "        filters=128,\n",
    "        kernel_size=(3, 3),\n",
    "        name=\"ssh_m3_det_context_conv3_2\",\n",
    "        strides=[1, 1],\n",
    "        padding=\"VALID\",\n",
    "        use_bias=True,\n",
    "    )(ssh_m3_det_context_conv3_2_pad)\n",
    "\n",
    "    ssh_m2_det_conv1_pad = ZeroPadding2D(padding=tuple([1, 1]))(ssh_c2_aggr_relu)\n",
    "\n",
    "    ssh_m2_det_conv1 = Conv2D(\n",
    "        filters=256,\n",
    "        kernel_size=(3, 3),\n",
    "        name=\"ssh_m2_det_conv1\",\n",
    "        strides=[1, 1],\n",
    "        padding=\"VALID\",\n",
    "        use_bias=True,\n",
    "    )(ssh_m2_det_conv1_pad)\n",
    "\n",
    "    ssh_m2_det_context_conv1_pad = ZeroPadding2D(padding=tuple([1, 1]))(ssh_c2_aggr_relu)\n",
    "\n",
    "    ssh_m2_det_context_conv1 = Conv2D(\n",
    "        filters=128,\n",
    "        kernel_size=(3, 3),\n",
    "        name=\"ssh_m2_det_context_conv1\",\n",
    "        strides=[1, 1],\n",
    "        padding=\"VALID\",\n",
    "        use_bias=True,\n",
    "    )(ssh_m2_det_context_conv1_pad)\n",
    "\n",
    "    ssh_m2_red_up = UpSampling2D(size=(2, 2), interpolation=\"nearest\", name=\"ssh_m2_red_up\")(\n",
    "        ssh_c2_aggr_relu\n",
    "    )\n",
    "\n",
    "    ssh_m3_det_context_conv3_2_bn = BatchNormalization(\n",
    "        epsilon=1.9999999494757503e-05, name=\"ssh_m3_det_context_conv3_2_bn\", trainable=False\n",
    "    )(ssh_m3_det_context_conv3_2)\n",
    "\n",
    "    ssh_m2_det_conv1_bn = BatchNormalization(\n",
    "        epsilon=1.9999999494757503e-05, name=\"ssh_m2_det_conv1_bn\", trainable=False\n",
    "    )(ssh_m2_det_conv1)\n",
    "\n",
    "    ssh_m2_det_context_conv1_bn = BatchNormalization(\n",
    "        epsilon=1.9999999494757503e-05, name=\"ssh_m2_det_context_conv1_bn\", trainable=False\n",
    "    )(ssh_m2_det_context_conv1)\n",
    "\n",
    "    x1_shape = tf.shape(ssh_m2_red_up)\n",
    "    x2_shape = tf.shape(ssh_m1_red_conv_relu)\n",
    "    offsets = [0, (x1_shape[1] - x2_shape[1]) // 2, (x1_shape[2] - x2_shape[2]) // 2, 0]\n",
    "    size = [-1, x2_shape[1], x2_shape[2], -1]\n",
    "    crop1 = tf.slice(ssh_m2_red_up, offsets, size, \"crop1\")\n",
    "\n",
    "    ssh_m3_det_concat = concatenate(\n",
    "        [ssh_m3_det_conv1_bn, ssh_m3_det_context_conv2_bn, ssh_m3_det_context_conv3_2_bn],\n",
    "        3,\n",
    "        name=\"ssh_m3_det_concat\",\n",
    "    )\n",
    "\n",
    "    ssh_m2_det_context_conv1_relu = ReLU(name=\"ssh_m2_det_context_conv1_relu\")(\n",
    "        ssh_m2_det_context_conv1_bn\n",
    "    )\n",
    "\n",
    "    plus1_v1 = Add()([ssh_m1_red_conv_relu, crop1])\n",
    "\n",
    "    ssh_m3_det_concat_relu = ReLU(name=\"ssh_m3_det_concat_relu\")(ssh_m3_det_concat)\n",
    "\n",
    "    ssh_m2_det_context_conv2_pad = ZeroPadding2D(padding=tuple([1, 1]))(\n",
    "        ssh_m2_det_context_conv1_relu\n",
    "    )\n",
    "\n",
    "    ssh_m2_det_context_conv2 = Conv2D(\n",
    "        filters=128,\n",
    "        kernel_size=(3, 3),\n",
    "        name=\"ssh_m2_det_context_conv2\",\n",
    "        strides=[1, 1],\n",
    "        padding=\"VALID\",\n",
    "        use_bias=True,\n",
    "    )(ssh_m2_det_context_conv2_pad)\n",
    "\n",
    "    ssh_m2_det_context_conv3_1_pad = ZeroPadding2D(padding=tuple([1, 1]))(\n",
    "        ssh_m2_det_context_conv1_relu\n",
    "    )\n",
    "\n",
    "    ssh_m2_det_context_conv3_1 = Conv2D(\n",
    "        filters=128,\n",
    "        kernel_size=(3, 3),\n",
    "        name=\"ssh_m2_det_context_conv3_1\",\n",
    "        strides=[1, 1],\n",
    "        padding=\"VALID\",\n",
    "        use_bias=True,\n",
    "    )(ssh_m2_det_context_conv3_1_pad)\n",
    "\n",
    "    ssh_c1_aggr_pad = ZeroPadding2D(padding=tuple([1, 1]))(plus1_v1)\n",
    "\n",
    "    ssh_c1_aggr = Conv2D(\n",
    "        filters=256,\n",
    "        kernel_size=(3, 3),\n",
    "        name=\"ssh_c1_aggr\",\n",
    "        strides=[1, 1],\n",
    "        padding=\"VALID\",\n",
    "        use_bias=True,\n",
    "    )(ssh_c1_aggr_pad)\n",
    "\n",
    "    face_rpn_cls_score_stride32 = Conv2D(\n",
    "        filters=4,\n",
    "        kernel_size=(1, 1),\n",
    "        name=\"face_rpn_cls_score_stride32\",\n",
    "        strides=[1, 1],\n",
    "        padding=\"VALID\",\n",
    "        use_bias=True,\n",
    "    )(ssh_m3_det_concat_relu)\n",
    "\n",
    "    inter_1 = concatenate(\n",
    "        [face_rpn_cls_score_stride32[:, :, :, 0], face_rpn_cls_score_stride32[:, :, :, 1]], axis=1\n",
    "    )\n",
    "    inter_2 = concatenate(\n",
    "        [face_rpn_cls_score_stride32[:, :, :, 2], face_rpn_cls_score_stride32[:, :, :, 3]], axis=1\n",
    "    )\n",
    "    final = tf.stack([inter_1, inter_2])\n",
    "    face_rpn_cls_score_reshape_stride32 = tf.transpose(\n",
    "        final, (1, 2, 3, 0), name=\"face_rpn_cls_score_reshape_stride32\"\n",
    "    )\n",
    "\n",
    "    face_rpn_bbox_pred_stride32 = Conv2D(\n",
    "        filters=8,\n",
    "        kernel_size=(1, 1),\n",
    "        name=\"face_rpn_bbox_pred_stride32\",\n",
    "        strides=[1, 1],\n",
    "        padding=\"VALID\",\n",
    "        use_bias=True,\n",
    "    )(ssh_m3_det_concat_relu)\n",
    "\n",
    "    face_rpn_landmark_pred_stride32 = Conv2D(\n",
    "        filters=20,\n",
    "        kernel_size=(1, 1),\n",
    "        name=\"face_rpn_landmark_pred_stride32\",\n",
    "        strides=[1, 1],\n",
    "        padding=\"VALID\",\n",
    "        use_bias=True,\n",
    "    )(ssh_m3_det_concat_relu)\n",
    "\n",
    "    ssh_m2_det_context_conv2_bn = BatchNormalization(\n",
    "        epsilon=1.9999999494757503e-05, name=\"ssh_m2_det_context_conv2_bn\", trainable=False\n",
    "    )(ssh_m2_det_context_conv2)\n",
    "\n",
    "    ssh_m2_det_context_conv3_1_bn = BatchNormalization(\n",
    "        epsilon=1.9999999494757503e-05, name=\"ssh_m2_det_context_conv3_1_bn\", trainable=False\n",
    "    )(ssh_m2_det_context_conv3_1)\n",
    "\n",
    "    ssh_c1_aggr_bn = BatchNormalization(\n",
    "        epsilon=1.9999999494757503e-05, name=\"ssh_c1_aggr_bn\", trainable=False\n",
    "    )(ssh_c1_aggr)\n",
    "\n",
    "    ssh_m2_det_context_conv3_1_relu = ReLU(name=\"ssh_m2_det_context_conv3_1_relu\")(\n",
    "        ssh_m2_det_context_conv3_1_bn\n",
    "    )\n",
    "\n",
    "    ssh_c1_aggr_relu = ReLU(name=\"ssh_c1_aggr_relu\")(ssh_c1_aggr_bn)\n",
    "\n",
    "    face_rpn_cls_prob_stride32 = Softmax(name=\"face_rpn_cls_prob_stride32\")(\n",
    "        face_rpn_cls_score_reshape_stride32\n",
    "    )\n",
    "\n",
    "    input_shape = [tf.shape(face_rpn_cls_prob_stride32)[k] for k in range(4)]\n",
    "    sz = tf.dtypes.cast(input_shape[1] / 2, dtype=tf.int32)\n",
    "    inter_1 = face_rpn_cls_prob_stride32[:, 0:sz, :, 0]\n",
    "    inter_2 = face_rpn_cls_prob_stride32[:, 0:sz, :, 1]\n",
    "    inter_3 = face_rpn_cls_prob_stride32[:, sz:, :, 0]\n",
    "    inter_4 = face_rpn_cls_prob_stride32[:, sz:, :, 1]\n",
    "    final = tf.stack([inter_1, inter_3, inter_2, inter_4])\n",
    "    face_rpn_cls_prob_reshape_stride32 = tf.transpose(\n",
    "        final, (1, 2, 3, 0), name=\"face_rpn_cls_prob_reshape_stride32\"\n",
    "    )\n",
    "\n",
    "    ssh_m2_det_context_conv3_2_pad = ZeroPadding2D(padding=tuple([1, 1]))(\n",
    "        ssh_m2_det_context_conv3_1_relu\n",
    "    )\n",
    "\n",
    "    ssh_m2_det_context_conv3_2 = Conv2D(\n",
    "        filters=128,\n",
    "        kernel_size=(3, 3),\n",
    "        name=\"ssh_m2_det_context_conv3_2\",\n",
    "        strides=[1, 1],\n",
    "        padding=\"VALID\",\n",
    "        use_bias=True,\n",
    "    )(ssh_m2_det_context_conv3_2_pad)\n",
    "\n",
    "    ssh_m1_det_conv1_pad = ZeroPadding2D(padding=tuple([1, 1]))(ssh_c1_aggr_relu)\n",
    "\n",
    "    ssh_m1_det_conv1 = Conv2D(\n",
    "        filters=256,\n",
    "        kernel_size=(3, 3),\n",
    "        name=\"ssh_m1_det_conv1\",\n",
    "        strides=[1, 1],\n",
    "        padding=\"VALID\",\n",
    "        use_bias=True,\n",
    "    )(ssh_m1_det_conv1_pad)\n",
    "\n",
    "    ssh_m1_det_context_conv1_pad = ZeroPadding2D(padding=tuple([1, 1]))(ssh_c1_aggr_relu)\n",
    "\n",
    "    ssh_m1_det_context_conv1 = Conv2D(\n",
    "        filters=128,\n",
    "        kernel_size=(3, 3),\n",
    "        name=\"ssh_m1_det_context_conv1\",\n",
    "        strides=[1, 1],\n",
    "        padding=\"VALID\",\n",
    "        use_bias=True,\n",
    "    )(ssh_m1_det_context_conv1_pad)\n",
    "\n",
    "    ssh_m2_det_context_conv3_2_bn = BatchNormalization(\n",
    "        epsilon=1.9999999494757503e-05, name=\"ssh_m2_det_context_conv3_2_bn\", trainable=False\n",
    "    )(ssh_m2_det_context_conv3_2)\n",
    "\n",
    "    ssh_m1_det_conv1_bn = BatchNormalization(\n",
    "        epsilon=1.9999999494757503e-05, name=\"ssh_m1_det_conv1_bn\", trainable=False\n",
    "    )(ssh_m1_det_conv1)\n",
    "\n",
    "    ssh_m1_det_context_conv1_bn = BatchNormalization(\n",
    "        epsilon=1.9999999494757503e-05, name=\"ssh_m1_det_context_conv1_bn\", trainable=False\n",
    "    )(ssh_m1_det_context_conv1)\n",
    "\n",
    "    ssh_m2_det_concat = concatenate(\n",
    "        [ssh_m2_det_conv1_bn, ssh_m2_det_context_conv2_bn, ssh_m2_det_context_conv3_2_bn],\n",
    "        3,\n",
    "        name=\"ssh_m2_det_concat\",\n",
    "    )\n",
    "\n",
    "    ssh_m1_det_context_conv1_relu = ReLU(name=\"ssh_m1_det_context_conv1_relu\")(\n",
    "        ssh_m1_det_context_conv1_bn\n",
    "    )\n",
    "\n",
    "    ssh_m2_det_concat_relu = ReLU(name=\"ssh_m2_det_concat_relu\")(ssh_m2_det_concat)\n",
    "\n",
    "    ssh_m1_det_context_conv2_pad = ZeroPadding2D(padding=tuple([1, 1]))(\n",
    "        ssh_m1_det_context_conv1_relu\n",
    "    )\n",
    "\n",
    "    ssh_m1_det_context_conv2 = Conv2D(\n",
    "        filters=128,\n",
    "        kernel_size=(3, 3),\n",
    "        name=\"ssh_m1_det_context_conv2\",\n",
    "        strides=[1, 1],\n",
    "        padding=\"VALID\",\n",
    "        use_bias=True,\n",
    "    )(ssh_m1_det_context_conv2_pad)\n",
    "\n",
    "    ssh_m1_det_context_conv3_1_pad = ZeroPadding2D(padding=tuple([1, 1]))(\n",
    "        ssh_m1_det_context_conv1_relu\n",
    "    )\n",
    "\n",
    "    ssh_m1_det_context_conv3_1 = Conv2D(\n",
    "        filters=128,\n",
    "        kernel_size=(3, 3),\n",
    "        name=\"ssh_m1_det_context_conv3_1\",\n",
    "        strides=[1, 1],\n",
    "        padding=\"VALID\",\n",
    "        use_bias=True,\n",
    "    )(ssh_m1_det_context_conv3_1_pad)\n",
    "\n",
    "    face_rpn_cls_score_stride16 = Conv2D(\n",
    "        filters=4,\n",
    "        kernel_size=(1, 1),\n",
    "        name=\"face_rpn_cls_score_stride16\",\n",
    "        strides=[1, 1],\n",
    "        padding=\"VALID\",\n",
    "        use_bias=True,\n",
    "    )(ssh_m2_det_concat_relu)\n",
    "\n",
    "    inter_1 = concatenate(\n",
    "        [face_rpn_cls_score_stride16[:, :, :, 0], face_rpn_cls_score_stride16[:, :, :, 1]], axis=1\n",
    "    )\n",
    "    inter_2 = concatenate(\n",
    "        [face_rpn_cls_score_stride16[:, :, :, 2], face_rpn_cls_score_stride16[:, :, :, 3]], axis=1\n",
    "    )\n",
    "    final = tf.stack([inter_1, inter_2])\n",
    "    face_rpn_cls_score_reshape_stride16 = tf.transpose(\n",
    "        final, (1, 2, 3, 0), name=\"face_rpn_cls_score_reshape_stride16\"\n",
    "    )\n",
    "\n",
    "    face_rpn_bbox_pred_stride16 = Conv2D(\n",
    "        filters=8,\n",
    "        kernel_size=(1, 1),\n",
    "        name=\"face_rpn_bbox_pred_stride16\",\n",
    "        strides=[1, 1],\n",
    "        padding=\"VALID\",\n",
    "        use_bias=True,\n",
    "    )(ssh_m2_det_concat_relu)\n",
    "\n",
    "    face_rpn_landmark_pred_stride16 = Conv2D(\n",
    "        filters=20,\n",
    "        kernel_size=(1, 1),\n",
    "        name=\"face_rpn_landmark_pred_stride16\",\n",
    "        strides=[1, 1],\n",
    "        padding=\"VALID\",\n",
    "        use_bias=True,\n",
    "    )(ssh_m2_det_concat_relu)\n",
    "\n",
    "    ssh_m1_det_context_conv2_bn = BatchNormalization(\n",
    "        epsilon=1.9999999494757503e-05, name=\"ssh_m1_det_context_conv2_bn\", trainable=False\n",
    "    )(ssh_m1_det_context_conv2)\n",
    "\n",
    "    ssh_m1_det_context_conv3_1_bn = BatchNormalization(\n",
    "        epsilon=1.9999999494757503e-05, name=\"ssh_m1_det_context_conv3_1_bn\", trainable=False\n",
    "    )(ssh_m1_det_context_conv3_1)\n",
    "\n",
    "    ssh_m1_det_context_conv3_1_relu = ReLU(name=\"ssh_m1_det_context_conv3_1_relu\")(\n",
    "        ssh_m1_det_context_conv3_1_bn\n",
    "    )\n",
    "\n",
    "    face_rpn_cls_prob_stride16 = Softmax(name=\"face_rpn_cls_prob_stride16\")(\n",
    "        face_rpn_cls_score_reshape_stride16\n",
    "    )\n",
    "\n",
    "    input_shape = [tf.shape(face_rpn_cls_prob_stride16)[k] for k in range(4)]\n",
    "    sz = tf.dtypes.cast(input_shape[1] / 2, dtype=tf.int32)\n",
    "    inter_1 = face_rpn_cls_prob_stride16[:, 0:sz, :, 0]\n",
    "    inter_2 = face_rpn_cls_prob_stride16[:, 0:sz, :, 1]\n",
    "    inter_3 = face_rpn_cls_prob_stride16[:, sz:, :, 0]\n",
    "    inter_4 = face_rpn_cls_prob_stride16[:, sz:, :, 1]\n",
    "    final = tf.stack([inter_1, inter_3, inter_2, inter_4])\n",
    "    face_rpn_cls_prob_reshape_stride16 = tf.transpose(\n",
    "        final, (1, 2, 3, 0), name=\"face_rpn_cls_prob_reshape_stride16\"\n",
    "    )\n",
    "\n",
    "    ssh_m1_det_context_conv3_2_pad = ZeroPadding2D(padding=tuple([1, 1]))(\n",
    "        ssh_m1_det_context_conv3_1_relu\n",
    "    )\n",
    "\n",
    "    ssh_m1_det_context_conv3_2 = Conv2D(\n",
    "        filters=128,\n",
    "        kernel_size=(3, 3),\n",
    "        name=\"ssh_m1_det_context_conv3_2\",\n",
    "        strides=[1, 1],\n",
    "        padding=\"VALID\",\n",
    "        use_bias=True,\n",
    "    )(ssh_m1_det_context_conv3_2_pad)\n",
    "\n",
    "    ssh_m1_det_context_conv3_2_bn = BatchNormalization(\n",
    "        epsilon=1.9999999494757503e-05, name=\"ssh_m1_det_context_conv3_2_bn\", trainable=False\n",
    "    )(ssh_m1_det_context_conv3_2)\n",
    "\n",
    "    ssh_m1_det_concat = concatenate(\n",
    "        [ssh_m1_det_conv1_bn, ssh_m1_det_context_conv2_bn, ssh_m1_det_context_conv3_2_bn],\n",
    "        3,\n",
    "        name=\"ssh_m1_det_concat\",\n",
    "    )\n",
    "\n",
    "    ssh_m1_det_concat_relu = ReLU(name=\"ssh_m1_det_concat_relu\")(ssh_m1_det_concat)\n",
    "    face_rpn_cls_score_stride8 = Conv2D(\n",
    "        filters=4,\n",
    "        kernel_size=(1, 1),\n",
    "        name=\"face_rpn_cls_score_stride8\",\n",
    "        strides=[1, 1],\n",
    "        padding=\"VALID\",\n",
    "        use_bias=True,\n",
    "    )(ssh_m1_det_concat_relu)\n",
    "\n",
    "    inter_1 = concatenate(\n",
    "        [face_rpn_cls_score_stride8[:, :, :, 0], face_rpn_cls_score_stride8[:, :, :, 1]], axis=1\n",
    "    )\n",
    "    inter_2 = concatenate(\n",
    "        [face_rpn_cls_score_stride8[:, :, :, 2], face_rpn_cls_score_stride8[:, :, :, 3]], axis=1\n",
    "    )\n",
    "    final = tf.stack([inter_1, inter_2])\n",
    "    face_rpn_cls_score_reshape_stride8 = tf.transpose(\n",
    "        final, (1, 2, 3, 0), name=\"face_rpn_cls_score_reshape_stride8\"\n",
    "    )\n",
    "\n",
    "    face_rpn_bbox_pred_stride8 = Conv2D(\n",
    "        filters=8,\n",
    "        kernel_size=(1, 1),\n",
    "        name=\"face_rpn_bbox_pred_stride8\",\n",
    "        strides=[1, 1],\n",
    "        padding=\"VALID\",\n",
    "        use_bias=True,\n",
    "    )(ssh_m1_det_concat_relu)\n",
    "\n",
    "    face_rpn_landmark_pred_stride8 = Conv2D(\n",
    "        filters=20,\n",
    "        kernel_size=(1, 1),\n",
    "        name=\"face_rpn_landmark_pred_stride8\",\n",
    "        strides=[1, 1],\n",
    "        padding=\"VALID\",\n",
    "        use_bias=True,\n",
    "    )(ssh_m1_det_concat_relu)\n",
    "\n",
    "    face_rpn_cls_prob_stride8 = Softmax(name=\"face_rpn_cls_prob_stride8\")(\n",
    "        face_rpn_cls_score_reshape_stride8\n",
    "    )\n",
    "\n",
    "    input_shape = [tf.shape(face_rpn_cls_prob_stride8)[k] for k in range(4)]\n",
    "    sz = tf.dtypes.cast(input_shape[1] / 2, dtype=tf.int32)\n",
    "    inter_1 = face_rpn_cls_prob_stride8[:, 0:sz, :, 0]\n",
    "    inter_2 = face_rpn_cls_prob_stride8[:, 0:sz, :, 1]\n",
    "    inter_3 = face_rpn_cls_prob_stride8[:, sz:, :, 0]\n",
    "    inter_4 = face_rpn_cls_prob_stride8[:, sz:, :, 1]\n",
    "    final = tf.stack([inter_1, inter_3, inter_2, inter_4])\n",
    "    face_rpn_cls_prob_reshape_stride8 = tf.transpose(\n",
    "        final, (1, 2, 3, 0), name=\"face_rpn_cls_prob_reshape_stride8\"\n",
    "    )\n",
    "\n",
    "    model = Model(\n",
    "        inputs=data,\n",
    "        outputs=[\n",
    "            face_rpn_cls_prob_reshape_stride32,\n",
    "            face_rpn_bbox_pred_stride32,\n",
    "            face_rpn_landmark_pred_stride32,\n",
    "            face_rpn_cls_prob_reshape_stride16,\n",
    "            face_rpn_bbox_pred_stride16,\n",
    "            face_rpn_landmark_pred_stride16,\n",
    "            face_rpn_cls_prob_reshape_stride8,\n",
    "            face_rpn_bbox_pred_stride8,\n",
    "            face_rpn_landmark_pred_stride8,\n",
    "        ],\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "6dce367f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Download weights function\n",
    "def download_retinaface_weights():\n",
    "    home = str(Path.home())\n",
    "    weights_dir = os.path.join(home, \".deepface\", \"weights\")\n",
    "    os.makedirs(weights_dir, exist_ok=True)\n",
    "    \n",
    "    weights_path = os.path.join(weights_dir, \"retinafacelib/retinaface.h5\")\n",
    "    url = \"https://github.com/serengil/deepface_models/releases/download/v1.0/retinaface.h5\"\n",
    "    \n",
    "    if not os.path.exists(weights_path):\n",
    "        print(\"Downloading RetinaFace weights...\")\n",
    "        gdown.download(url, weights_path, quiet=False)\n",
    "        print(f\"Weights downloaded to: {weights_path}\")\n",
    "    \n",
    "    return weights_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "a2fca889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Load weights into model\n",
    "def load_weights_into_model(model, weights_path):\n",
    "    \"\"\"Load the downloaded .h5 weights into the model\"\"\"\n",
    "    if not os.path.exists(weights_path):\n",
    "        raise ValueError(f\"Weights file not found: {weights_path}\")\n",
    "    \n",
    "    print(\"Loading weights into model...\")\n",
    "    model.load_weights(weights_path)\n",
    "    print(\"Weights loaded successfully!\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "3596b52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Complete workflow\n",
    "def create_retinaface_model():\n",
    "    \"\"\"Complete workflow to create RetinaFace model with pre-trained weights\"\"\"\n",
    "    \n",
    "    # 1. Download weights\n",
    "    # weights_path = download_retinaface_weights()\n",
    "    weights_path = \"retinafacelib/retinaface.h5\"\n",
    "    \n",
    "    # 2. Build model architecture\n",
    "    print(\"Building RetinaFace model architecture...\")\n",
    "    model = build_retinaface_model()\n",
    "    \n",
    "    # 3. Load pre-trained weights\n",
    "    model = load_weights_into_model(model, weights_path)\n",
    "    \n",
    "    # 4. Convert to TensorFlow function for optimization (as in original code)\n",
    "    model_func = tf.function(\n",
    "        model,\n",
    "        input_signature=(tf.TensorSpec(shape=[None, None, None, 3], dtype=np.float32),)\n",
    "    )\n",
    "    \n",
    "    return model_func\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fbc2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_faces(\n",
    "    img_path: Union[str, np.ndarray],\n",
    "    threshold: float = 0.9,\n",
    "    model: Optional[Model] = None,\n",
    "    allow_upscaling: bool = True,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Detect the facial area for a given image\n",
    "    Args:\n",
    "        img_path (str or numpy array): given image\n",
    "        threshold (float): threshold for detection\n",
    "        model (Model): pre-trained model can be given\n",
    "        allow_upscaling (bool): allowing up-scaling\n",
    "    Returns:\n",
    "        detected faces as:\n",
    "        {\n",
    "            \"face_1\": {\n",
    "                \"score\": 0.9993440508842468,\n",
    "                \"facial_area\": [155, 81, 434, 443],\n",
    "                \"landmarks\": {\n",
    "                    \"right_eye\": [257.82974, 209.64787],\n",
    "                    \"left_eye\": [374.93427, 251.78687],\n",
    "                    \"nose\": [303.4773, 299.91144],\n",
    "                    \"mouth_right\": [228.37329, 338.73193],\n",
    "                    \"mouth_left\": [320.21982, 374.58798]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    \"\"\"\n",
    "    \n",
    "    resp = {}\n",
    "    img = preprocess.get_image(img_path)\n",
    "\n",
    "    # ---------------------------\n",
    "\n",
    "    if model is None:\n",
    "        print(\"Model not available\")\n",
    "\n",
    "    # ---------------------------\n",
    "\n",
    "    nms_threshold = 0.4\n",
    "    decay4 = 0.5\n",
    "\n",
    "    _feat_stride_fpn = [32, 16, 8]\n",
    "\n",
    "    _anchors_fpn = {\n",
    "        \"stride32\": np.array(\n",
    "            [[-248.0, -248.0, 263.0, 263.0], [-120.0, -120.0, 135.0, 135.0]], dtype=np.float32\n",
    "        ),\n",
    "        \"stride16\": np.array(\n",
    "            [[-56.0, -56.0, 71.0, 71.0], [-24.0, -24.0, 39.0, 39.0]], dtype=np.float32\n",
    "        ),\n",
    "        \"stride8\": np.array([[-8.0, -8.0, 23.0, 23.0], [0.0, 0.0, 15.0, 15.0]], dtype=np.float32),\n",
    "    }\n",
    "\n",
    "    _num_anchors = {\"stride32\": 2, \"stride16\": 2, \"stride8\": 2}\n",
    "\n",
    "    # ---------------------------\n",
    "\n",
    "    proposals_list = []\n",
    "    scores_list = []\n",
    "    landmarks_list = []\n",
    "    im_tensor, im_info, im_scale = preprocess.preprocess_image(img, allow_upscaling)\n",
    "    net_out = model(im_tensor)\n",
    "    net_out = [elt.numpy() for elt in net_out]\n",
    "    sym_idx = 0\n",
    "\n",
    "    for _, s in enumerate(_feat_stride_fpn):\n",
    "        # _key = f\"stride{s}\"\n",
    "        scores = net_out[sym_idx]\n",
    "        scores = scores[:, :, :, _num_anchors[f\"stride{s}\"] :]\n",
    "\n",
    "        bbox_deltas = net_out[sym_idx + 1]\n",
    "        height, width = bbox_deltas.shape[1], bbox_deltas.shape[2]\n",
    "\n",
    "        A = _num_anchors[f\"stride{s}\"]\n",
    "        K = height * width\n",
    "        anchors_fpn = _anchors_fpn[f\"stride{s}\"]\n",
    "        anchors = postprocess.anchors_plane(height, width, s, anchors_fpn)\n",
    "        anchors = anchors.reshape((K * A, 4))\n",
    "        scores = scores.reshape((-1, 1))\n",
    "\n",
    "        bbox_stds = [1.0, 1.0, 1.0, 1.0]\n",
    "        bbox_pred_len = bbox_deltas.shape[3] // A\n",
    "        bbox_deltas = bbox_deltas.reshape((-1, bbox_pred_len))\n",
    "        bbox_deltas[:, 0::4] = bbox_deltas[:, 0::4] * bbox_stds[0]\n",
    "        bbox_deltas[:, 1::4] = bbox_deltas[:, 1::4] * bbox_stds[1]\n",
    "        bbox_deltas[:, 2::4] = bbox_deltas[:, 2::4] * bbox_stds[2]\n",
    "        bbox_deltas[:, 3::4] = bbox_deltas[:, 3::4] * bbox_stds[3]\n",
    "        proposals = postprocess.bbox_pred(anchors, bbox_deltas)\n",
    "\n",
    "        proposals = postprocess.clip_boxes(proposals, im_info[:2])\n",
    "\n",
    "        if s == 4 and decay4 < 1.0:\n",
    "            scores *= decay4\n",
    "\n",
    "        scores_ravel = scores.ravel()\n",
    "        order = np.where(scores_ravel >= threshold)[0]\n",
    "        proposals = proposals[order, :]\n",
    "        scores = scores[order]\n",
    "\n",
    "        proposals[:, 0:4] /= im_scale\n",
    "        proposals_list.append(proposals)\n",
    "        scores_list.append(scores)\n",
    "\n",
    "        landmark_deltas = net_out[sym_idx + 2]\n",
    "        landmark_pred_len = landmark_deltas.shape[3] // A\n",
    "        landmark_deltas = landmark_deltas.reshape((-1, 5, landmark_pred_len // 5))\n",
    "        landmarks = postprocess.landmark_pred(anchors, landmark_deltas)\n",
    "        landmarks = landmarks[order, :]\n",
    "\n",
    "        landmarks[:, :, 0:2] /= im_scale\n",
    "        landmarks_list.append(landmarks)\n",
    "        sym_idx += 3\n",
    "\n",
    "    proposals = np.vstack(proposals_list)\n",
    "\n",
    "    if proposals.shape[0] == 0:\n",
    "        return resp\n",
    "\n",
    "    scores = np.vstack(scores_list)\n",
    "    scores_ravel = scores.ravel()\n",
    "    order = scores_ravel.argsort()[::-1]\n",
    "\n",
    "    proposals = proposals[order, :]\n",
    "    scores = scores[order]\n",
    "    landmarks = np.vstack(landmarks_list)\n",
    "    landmarks = landmarks[order].astype(np.float32, copy=False)\n",
    "\n",
    "    pre_det = np.hstack((proposals[:, 0:4], scores)).astype(np.float32, copy=False)\n",
    "\n",
    "    # nms = cpu_nms_wrapper(nms_threshold)\n",
    "    # keep = nms(pre_det)\n",
    "    keep = postprocess.cpu_nms(pre_det, nms_threshold)\n",
    "\n",
    "    det = np.hstack((pre_det, proposals[:, 4:]))\n",
    "    det = det[keep, :]\n",
    "    landmarks = landmarks[keep]\n",
    "\n",
    "    for idx, face in enumerate(det):\n",
    "        label = \"face_\" + str(idx + 1)\n",
    "        resp[label] = {}\n",
    "        resp[label][\"score\"] = face[4]\n",
    "\n",
    "        resp[label][\"facial_area\"] = list(face[0:4].astype(int))\n",
    "\n",
    "        resp[label][\"landmarks\"] = {}\n",
    "        resp[label][\"landmarks\"][\"right_eye\"] = list(landmarks[idx][0])\n",
    "        resp[label][\"landmarks\"][\"left_eye\"] = list(landmarks[idx][1])\n",
    "        resp[label][\"landmarks\"][\"nose\"] = list(landmarks[idx][2])\n",
    "        resp[label][\"landmarks\"][\"mouth_right\"] = list(landmarks[idx][3])\n",
    "        resp[label][\"landmarks\"][\"mouth_left\"] = list(landmarks[idx][4])\n",
    "\n",
    "    return resp\n",
    "\n",
    "\n",
    "def extract_faces(\n",
    "    img_path: Union[str, np.ndarray],\n",
    "    threshold: float = 0.9,\n",
    "    model: Optional[Model] = None,\n",
    "    align: bool = False,\n",
    "    allow_upscaling: bool = True,\n",
    "    expand_face_area: int = 0,\n",
    "    target_size: Optional[Tuple[int, int]] = None,\n",
    "    min_max_norm: bool = True,\n",
    ") -> List[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Extract detected and aligned faces\n",
    "    Args:\n",
    "        img_path (str or numpy): given image\n",
    "        threshold (float): detection threshold\n",
    "        model (Model): pre-trained model can be passed to the function\n",
    "        align (bool): enable or disable alignment\n",
    "        allow_upscaling (bool): allowing up-scaling\n",
    "        expand_face_area (int): expand detected facial area with a percentage\n",
    "        target_size (optional tuple): resize the image by padding it with black pixels\n",
    "            to fit the specified dimensions. default is None\n",
    "        min_max_norm (bool): set this to True if you want to normalize image in [0, 1].\n",
    "            this is only running when target_size is not none.\n",
    "            for instance, matplotlib expects inputs in this scale. (default is True)\n",
    "    Returns:\n",
    "        result (List[np.ndarray]): list of extracted faces\n",
    "    \"\"\"\n",
    "    resp = []\n",
    "\n",
    "    # ---------------------------\n",
    "\n",
    "    img = preprocess.get_image(img_path)\n",
    "\n",
    "    # ---------------------------\n",
    "\n",
    "    obj = detect_faces(\n",
    "        img_path=img, threshold=threshold, model=model, allow_upscaling=allow_upscaling\n",
    "    )\n",
    "\n",
    "    if not isinstance(obj, dict):\n",
    "        return resp\n",
    "\n",
    "    for _, identity in obj.items():\n",
    "        facial_area = identity[\"facial_area\"]\n",
    "        rotate_angle = 0\n",
    "        rotate_direction = 1\n",
    "\n",
    "        x = facial_area[0]\n",
    "        y = facial_area[1]\n",
    "        w = facial_area[2] - x\n",
    "        h = facial_area[3] - y\n",
    "\n",
    "        if expand_face_area > 0:\n",
    "            expanded_w = w + int(w * expand_face_area / 100)\n",
    "            expanded_h = h + int(h * expand_face_area / 100)\n",
    "\n",
    "            # overwrite facial area\n",
    "            x = max(0, x - int((expanded_w - w) / 2))\n",
    "            y = max(0, y - int((expanded_h - h) / 2))\n",
    "            w = min(img.shape[1] - x, expanded_w)\n",
    "            h = min(img.shape[0] - y, expanded_h)\n",
    "\n",
    "        facial_img = img[y : y + h, x : x + w]\n",
    "\n",
    "        if align is True:\n",
    "            landmarks = identity[\"landmarks\"]\n",
    "            left_eye = landmarks[\"left_eye\"]\n",
    "            right_eye = landmarks[\"right_eye\"]\n",
    "            nose = landmarks[\"nose\"]\n",
    "            # mouth_right = landmarks[\"mouth_right\"]\n",
    "            # mouth_left = landmarks[\"mouth_left\"]\n",
    "\n",
    "            # notice that left eye of one is seen on the right from your perspective\n",
    "            aligned_img, rotate_angle, rotate_direction = postprocess.alignment_procedure(\n",
    "                img=img, left_eye=right_eye, right_eye=left_eye, nose=nose\n",
    "            )\n",
    "\n",
    "            # find new facial area coordinates after alignment\n",
    "            rotated_x1, rotated_y1, rotated_x2, rotated_y2 = postprocess.rotate_facial_area(\n",
    "                (x, y, x + w, y + h), rotate_angle, rotate_direction, (img.shape[0], img.shape[1])\n",
    "            )\n",
    "            facial_img = aligned_img[\n",
    "                int(rotated_y1) : int(rotated_y2), int(rotated_x1) : int(rotated_x2)\n",
    "            ]\n",
    "\n",
    "        if target_size is not None:\n",
    "            facial_img = postprocess.resize_image(\n",
    "                img=facial_img, target_size=target_size, min_max_norm=min_max_norm\n",
    "            )\n",
    "\n",
    "        # to rgb\n",
    "        facial_img = facial_img[:, :, ::-1]\n",
    "\n",
    "        resp.append(facial_img)\n",
    "\n",
    "    return resp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98c1126",
   "metadata": {},
   "source": [
    "# Model def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "998f9977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building RetinaFace model architecture...\n",
      "Loading weights into model...\n",
      "Weights loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "model = create_retinaface_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "74479950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading RetinaFace model...\n",
      "[INFO] Saved: crops\\WhatsApp Image 2025-07-30 at 16.57.34_face1.jpg\n",
      "[INFO] Saved: crops\\WhatsApp Image 2025-07-30 at 16.57.37_face1.jpg\n",
      "[INFO] Saved: crops\\WhatsApp Image 2025-07-30 at 16.57.47_face1.jpg\n",
      "[INFO] Saved: crops\\WhatsApp Image 2025-07-30 at 16.57.48_face1.jpg\n",
      "[INFO] Selesai.\n"
     ]
    }
   ],
   "source": [
    "INPUT_PATH = \"images\"      # bisa folder atau file tunggal\n",
    "OUTPUT_DIR = \"crops\"       # folder hasil crop\n",
    "CONF_THRESHOLD = 0.9       # tingkat keyakinan deteksi wajah \n",
    "\n",
    "# Buat folder output jika belum ada\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Load RetinaFace model\n",
    "print(\"[INFO] Loading RetinaFace model...\")\n",
    "\n",
    "def process_image(image_path):\n",
    "    \"\"\"\n",
    "    Deteksi wajah dan simpan hasil crop ke folder OUTPUT_DIR.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        faces = extract_faces(\n",
    "            img_path=image_path,\n",
    "            model=model,\n",
    "        )\n",
    "\n",
    "        for i, face in enumerate(faces):\n",
    "            # print(f\"Face {i+1} area:\", face[\"facial_area\"])\n",
    "            # cv2.imwrite(f\"crop_{i+1}.jpg\", cv2.cvtColor(face[\"face\"], cv2.COLOR_RGB2BGR))\n",
    "\n",
    "            filename = os.path.splitext(os.path.basename(image_path))[0]\n",
    "            save_path = os.path.join(OUTPUT_DIR, f\"{filename}_face{i+1}.jpg\")\n",
    "\n",
    "            # Perbaiki tipe data: float (0–1) -> uint8 (0–255)\n",
    "            if face.dtype != \"uint8\":\n",
    "                face = (face * 255).astype(\"uint8\")\n",
    "\n",
    "            # Konversi RGB -> BGR sebelum simpan\n",
    "            face_bgr = cv2.cvtColor(face, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "            cv2.imwrite(save_path, face_bgr)\n",
    "            print(f\"[INFO] Saved: {save_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Gagal memproses {image_path}: {e}\")\n",
    "\n",
    "# Proses folder atau file tunggal\n",
    "if os.path.isdir(INPUT_PATH):\n",
    "    for fname in os.listdir(INPUT_PATH):\n",
    "        fpath = os.path.join(INPUT_PATH, fname)\n",
    "        if fname.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "            process_image(fpath)\n",
    "else:\n",
    "    process_image(INPUT_PATH)\n",
    "\n",
    "print(\"[INFO] Selesai.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
